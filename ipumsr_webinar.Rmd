---
title: "Enhancing reproducibility with the IPUMS API and the ipumsr package"
author: "Derek Burk, Dan Ehrlich, & Kara Fisher"
date: "4/11/2022"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  collapse = TRUE,
  comment = "#>"
)

hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- gsub(
    "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", 
    "*\\1", 
    x,
    perl = TRUE
  )
  hook_source(x, options)
})

options(width = 50)
```

<style>
.derek-pet {
  background-color: #000000;
  background-image: url(images/derek2.jpg);
  background-size: cover;
  width: 120px;
  height: 160px;
  position: absolute;
  left: 125px;
  top: 150px;
}
</style>


<style>
.dan-pet1 {
  background-color: #d3d3d3;
  background-image: url(images/felix.jpg);
  background-size: cover;
  width: 120px;
  height: 160px;
  position: absolute;
  left: 305px;
  top: 225px;
  
}
</style>


<style>
.dan-pet2 {
  background-color: #000000;
  background-image: url(images/luna.jpg);
  background-size: cover;
  width: 120px;
  height: 120px;  
  position: absolute;
  left: 450px;
  top: 250px;
}
</style>


<style>
.kara-pet1 {
  background-color: #000000;
  background-image: url(images/Ru.jpg);
  background-size: cover;
  width: 120px;
  height: 160px;
  position: absolute;
  left: 600px;
  bottom: 200px;
}
</style>

<style>
.kara-pet2 {
  background-color: #000000;
  background-image: url(images/harvey.jpg);
  background-size: cover;
  width: 120px;
  height: 160px;
  position: absolute;
  left: 730px;
  bottom: 200px;
}
</style>




<style>
.grid-logo {
  background-image: url(images/grid.png);
  background-size: cover;
  width: 400px;
  height: 400px;
  position: absolute;
  right: 75px;

}
</style>


<style>
.isrdi-logo {
  background-image: url(images/isrdi2.jpg);
  background-size: cover;
  height: 186px;
  width: 604px;
  position: absolute;
  bottom: 15px;
  left: 15;
}
</style>



<style>
.names {
  font-size: 1.5em;
}
</style>

<style>
  .greyed-out {
    color: #D3D3D3;
    text-indent: 20px;
  }
</style>

<style>
  .strong {
    color: #000000;
    text-indent: 20px;
  }
</style>

# Who we are


.isrdi-logo[

]

--

.left[
Derek Burk, PhD 

Sociology
]

--

.center[
Dan Ehrlich, MA

Anthropology


]

--

.right[


Kara Fisher,
MPH

Public Health

]


???
(Name, pronouns, academic field, time with IPUMS)

So, my name is Derek Burk, I use he/him pronouns, I've been working on the
IPUMS International team for the past four years, and my academic training is in
sociology.


---

# Who we are

.isrdi-logo[]


.derek-pet[

]

.dan-pet1[]

.dan-pet2[]

.kara-pet1[]
.kara-pet2[]


???

Rather than our faces, we thought everyone would appreciate pets.

**A FEW NOTES**

This presentation will be recorded and will be available along with slides so don't worry about trying to take notes.

You can always reach out to through our **github**, or via **user support**.

---

# Overview

1. What is IPUMS?

--

2. What is ipumsr, and why use it?

--

3. How to create a data extract

--

4. Reading data into R

--

5. Exploring and manipulating metadata

--

6. Intro to the IPUMS USA API

--

7. IPUMS API use cases

--

8. Q & A

---




# Overview


.greyed-out[


.strong[`1.` What is IPUMS?]

`2.` What is ipumsr, and why use it?

`3.` How to create a data extract

`4.` Reading data into R

`5.` Exploring and manipulating metadata

`6.` Intro to the IPUMS USA API

`7.` IPUMS API use cases

`8.` Q & A



]


---


class: center

# What is IPUMS?

--

## IPUMS is **data**


## from censuses and surveys around the world,


## **harmonized** across space and time,


## thoroughly documented,


## and available for free at ipums.org


???



ipums has grown substantially since its first beta release in 1993, 

started with **US census data** has grown to include 9 collections


---

# Harmonization



```{r echo=FALSE, fig.alt="Table illustrating IPUMS harmonization process.", fig.height=3, fig.width=5}
knitr::include_graphics("images/harmonization_4.png")
```


---
# 
![IPUMS US Project logo](images/usa.jpg)



- U.S. Census and American Community Survey **microdata** from 1850 to the present.

- 180,755,919 unique person records from decennial census and American Community Survey.

- 191,983,898 historical person records from full count decennial census from 1850-1940 (1890 census lost due to fire).

- https://usa.ipums.org/usa/

???



---
# 
![IPUMS CPS project logo](images/cps.jpg)



- Current Population Survey **microdata** from 1962 to the present.

- Monthly labor force surveys and supplements.

- https://cps.ipums.org/cps/


---
# 
![IPUMS Health Surveys project logo](images/health.jpg)





- Health **survey** data from the National Health Interview Survey (NHIS) from the 1960s to the present and the Medical Expenditure Panel Survey (MEPS) from 1996.

- Supplements on cost of healthcare.

- https://healthsurveys.ipums.org/
???




---
# 
![IPUMS Higher Ed project logo](images/highered.jpg)




- Scientists and Engineers Statistical Data System (SESTAT), the leading surveys for studying the science and engineering (STEM) workforce in the United States

- Data from the National Surveys of College Graduates (NSCG), Recent College Graduates (NSRCG) and Doctorate Recipients (SDR) are integrated from 1993 to the present.

- https://highered.ipums.org/highered/



---
# 
![IPUMS international project logo](images/int.jpg)




- Census **microdata** covering 103 countries from 1960 to the present 


- International historic **microdata** from the 19th and early 20th centuries available for some samples.



- Labor Force surveys provide high resolution **microdata** about work conditions

  - Administered quarterly (usually) with records going back at least 10 years (usually)
  - Currently available for Italy (2011-2020), Spain (2005-2020), and Mexico (2005-2020)

- https://international.ipums.org/international/



---
# 
![IPUMS international project logo](images/int.jpg)



.pull-left[

- “Climate-Induced migration and unemployment in middle-income Africa”

- Valerie Mueller, Clark Gray, and Douglas Hopping
]

.pull-right[
![Maps of migration, education, temperature, and precipitation for 3 counties.](images/example_international.png)
]





---
# 
![IPUMS Global Health project logo](images/dhs.jpg)





- Demographic and Health Surveys (DHS) provide integrated **microdata** for analysis across time and space.
  - From the 1980s to the present.
  - Covering Africa and South Asia
  
- Performance Monitoring for Action (PMA) surveys
  - Focus on fertility, contraception, hygiene, and health
  - Administered frequently to monitor trends in select high-fertility countries.
  
  - https://globalhealth.ipums.org/

---



# 
![IPUMS Global Health project logo](images/dhs.jpg)




.pull-left[
![alluvial plot showing which individuals were using family planning at two points in time](images/example_global_health.png)
]

.pull-right[
  - https://tech.popdata.org/pma-data-hub/
]

???

Check out blog using IPUMS Global Health Data



---



.pull-left[ 
![IPUMS NHGIS project logo](images/nhgis.jpg)

]


.pull-right[


![IPUMS IHGIS project logo](images/ihgis.jpg)
]


.pull-left[ 

- **NHGIS** Shapefiles for all levels of US geography including tracts, from 1790 to the present


- https://www.nhgis.org/
]


.pull-right[


- **IHGIS** Shapefiles for admin level 2

- https://ihgis.ipums.org/


]

- Summary tables and time series of population, housing, agriculture, and economic data 




---

![IPUMS NHGIS project logo](images/nhgis.jpg)


- “Migrant Selection and Sorting during the Great American Drought.” 
- Christopher Sichko




.pull-left[
![map showing extent of drought](images/example_nhgis1.png)



]

.pull-right[



![map showing educational achievement by region](images/example_nhgis2b.png)


]




---


# 
![IPUMS Time Use project logo](images/time_use.jpg)



- Historical and contemporary time use data from 1965 to the present.

- Extensive time diary data from respondents in the US and *7* other countries.

- https://timeuse.ipums.org/


---

# 
![IPUMS Time Use project logo](images/time_use.jpg)
.pull-left[
- Nathan Yau of Flowing Data

- http://flowingdata.com/projects/2015/timeuse-simulation/
]

.pull-right[
!["Visualization of individuals (dots) moving between activity categories."](images/example_time_use.png)

]


---

# So what is IPUMS?


.grid-logo[

]

.pull-left[

- IPUMS is **a lot** of data

- Individual-level microdata

- Summarized tabular data

- GIS shapefiles

- Consistent and extensively documented **metadata**


]


???

So ipums really is **data** and a whole lot of it. These 9 different projects interact with different types of data and at different scales but they are united in the use of metadata that helps contextualize the data

*So I know you're asking yourselves..*
---

# So what is IPUMS?


.grid-logo[

]


.pull-left[
- IPUMS is **a lot** of data

- Individual-level microdata

- Summarized tabular data

- GIS shapefiles

- Consistent and extensively documented **metadata**

- *How can I work with all this IPUMS data?*
]




???



---


# Overview

.greyed-out[

`1.` What is IPUMS?

.strong[`2.` What is ipumsr, and why use it?]

`3.` How to create a data extract

`4.` Reading data into R

`5.` Exploring and manipulating metadata

`6.` Intro to the IPUMS USA API

`7.` IPUMS API use cases

`8.` Q & A


]


---


# What is ipumsr?

<style>
.ipumsr-logo {
  background-image: url(images/ipumsr.png);
  background-size: cover;
  height: 298px;
  width: 257px;
  position: absolute;
  right: 50px;
  bottom: 50px;
}
</style>


.ipumsr-logo[

]

- R package developed by Greg Freedman Ellis


- Released in 2017


- Over 100,000 CRAN downloads


- Includes functions for
  - Reading IPUMS data
  - Exploring and manipulating IPUMS metadata
  - **SOON**: Interacting with the IPUMS API
    
    
???
(Metadata such as value labels, variable labels, and detailed variable 
descriptions.)

Initial API support will be for IPUMS USA, with more projects to follow soon.



---

# Why use ipumsr?

- One package for IPUMS microdata, aggregate data, and geography



- Specialized functions for viewing and manipulating IPUMS metadata



- Bundled how-to guides (vignettes)



- Potential to add more features (e.g. API support); let us know what you want!
  - File an issue at https://github.com/ipums/ipumsr/issues
  - Email ipums+cran@umn.edu

???

Regarding "One package": Without ipumsr, you'd need to use a variety of
different approaches from different packages to read in and explore IPUMS


**microdata** from IPUMS: USA, CPS, and International, IPUMS

**aggregate data** (from NHGIS or IHGIS), 
and **IPUMS shapefiles**. ipumsr provides one
package with a consistent interface for working with all these different types
of IPUMS data.

*a one stop shop that makes it easy to work with ipums data*

Regarding "More features": The aforementioned IPUMS API support will be the next
big feature. Another potential new feature is adding tools for properly handling
survey weights. Let us know what would be helpful to you via GitHub or email.

---
class: center, middle

# To run the code in this presentation

???

Repeat: slides/recording will be available. To run this code yourself you can clone/download the repo from our github. This provides the data extracts you'll need. 

However, you may need to install some additional packages used in the example code below, as shown on the next slide.

---

.pull-left[
# Install R packages (as needed)

```{r, eval = FALSE}
install.packages("ipumsr")

## Tidyverse
install.packages("dplyr")
install.packages("ggplot2")
install.packages("stringr")
install.packages("purrr")

## HTML tables
install.packages("DT")

## GitHub helper functions
install.packages("usethis")
```

]


.pull-right[
# Load R packages (each time)
```{r, eval = FALSE}
library(ipumsr)

## Tidyverse
library(dplyr)
library(ggplot2)
library(stringr)
library(purrr)

## HTML tables
library(DT)

## GitHub helper functions
library(usethis)
```

]

```{r, echo = FALSE, include=FALSE}
library(ipumsr)

## Tidyverse
library(dplyr)
library(ggplot2)
library(stringr)
library(purrr)

## HTML tables
library(DT)

## GitHub helper functions
library(usethis)
```


???

reminder, installing packages only needs to happen once, but some packages do update frequently so it can be a good idea to re-install once in a while


---


# Overview

.greyed-out[



`1.` What is IPUMS?

`2.` What is ipumsr, and why use it?

.strong[`3.` How to create a data extract]

`4.` Reading data into R

`5.` Exploring and manipulating metadata

`6.` Intro to the IPUMS USA API

`7.` IPUMS API use cases

`8.` Q & A

]

???

Dan passes to Derek. Derek switches to a web demo of extract system.

---


# Overview

.greyed-out[



`1.` What is IPUMS?

`2.` What is ipumsr, and why use it?

`3.` How to create a data extract

.strong[`4.` Reading data into R]

`5.` Exploring and manipulating metadata

`6.` Intro to the IPUMS USA API

`7.` IPUMS API use cases

`8.` Q & A


]


???

Website demo ends, back to slides. Derek passes to Kara.

---

# Downloading your data extract

```{r echo=FALSE, fig.alt="Screenshot of IPUMS data download page with overlaid instructions: 1. Click the 'Download .DAT' link to download the data. 2. Right click the 'DDI' link. 3. In the right-click menu, choose 'Save link as' in Firefox or Chrome, or 'Download Linked File' in Safari."}
knitr::include_graphics("images/microdata_annotated_screenshot.png")
```

--

- You must download both the data and DDI codebook

- Save both files in the same folder

???

Kind of confusing how to save the DDI/.xml file. THIS IS HOW.
DDI is EXTREMELY important, as it contains all the instructions regarding the METADATA

Once your extract is complete, download the data file and the DDI. Downloading
the DDI is a little bit different depending on your browser. On most browsers
you should right-click the file and select “Save As…”. If this saves a file with
a .xml file extension, then you should be ready. However, Safari users must
select “Download Linked File” instead of “Download Linked File As”. On Safari,
selecting the wrong version of these two will download a file with a .html file
extension instead of a .xml extension.

In case anyone was curious, DDI stands for "Data Documentation Initiative" --
the DDI project sets standards for documenting datasets, and the codebooks for
most IPUMS projects follow this standard.

Make sure to save the data and DDI files in the same location.

---

# Downloading your data extract

- Optional: "R" link contains code to read in your data with ipumsr

```{r echo=FALSE, fig.alt="Screenshot of IPUMS data download page with the 'R' link highlighted"}
knitr::include_graphics("images/download_screenshot_2.png")
```


???
The links under "Command Files" contain program-specific code for reading in the
data. The R one contains the code we'll show on the next slide.

This helper code checks that you have ipumsr installed, and if you do, it reads
in the DDI codebook and data into separate objects. 

---

# Read in the data

```{r include=FALSE}
if (!file.exists("prcs_migration_extract.xml")) {
  # Load extract definition from JSON
  prcs_migration_extract <- define_extract_from_json(
    "prcs_migration_extract.json",
    "usa"
  )
  # Submit, wait for, and download extract
  ddi_filename <- submit_extract(prcs_migration_extract) %>% 
    wait_for_extract() %>% 
    download_extract() %>% 
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names
  file.rename(ddi_filename, "prcs_migration_extract.xml")
  file.rename(data_filename, "prcs_migration_extract.dat.gz")
}

ddi <- read_ipums_ddi("prcs_migration_extract.xml")
data <- read_ipums_micro(ddi, data_file = "prcs_migration_extract.dat.gz")
```


- Using functions `read_ipums_ddi()` and `read_ipums_micro()`
```{r eval=FALSE}
ddi <- read_ipums_ddi("usa_00013.xml")

data <- read_ipums_micro(ddi)
```

- Note: supply the codebook, *not* the data file, to `read_ipums_micro()`

???
So you've downloaded both the data and the DDI codebook, and saved them in the 
same folder. Here's how you actually read the data into R.

The first option, and the one I'd recommend, is to read the DDI codebook into an 
object named "ddi" using the `read_ipums_ddi()` function, and then supply that 
ddi object to `read_ipums_micro()`.

Notice that we are supplying the codebook (either already read in
or as a file path), not the data file, to `read_ipums_micro()`. The fact that we
don't point to the actual data file when reading in the data might confusing. 

Want to emphasize that we are pointing to the ddi - not the data file - to read 
it in.

***Change the slides below to just show what's in a ddi + data file, so it
makes sense why we need both (streamlined version of what we did before)

---

# The data file is just raw ingredients

```{r echo=FALSE, comment = ''}
readLines("prcs_migration_extract.dat.gz", n = 10) %>% 
  substr(1,70) %>% 
  paste0(collapse = "\n") %>% 
  cat() 

```
???
Looking at the first ten lines of our example data file, it makes sense why you 
can't just tell R "read this data file into a data.frame". The data file itself
doesn't contain any information about what the variables are called or which 
numbers on each line correspond to which variables, let alone what the meaning 
of those values might be.

(In case you're wondering what those blanks are, that's a geography variable 
that is not available in some years of our data.)

---

# The DDI codebook is the recipe

```{r, results='markup'}
names(ddi)
```
--

```{r}
ddi$file_name
```

???
The DDI codebook tells you what to do with the ingredients in the data file. It
contains a bunch of information about your extract, including the name of the
data file, which is why you don't need to supply the path to the data file if
you saved the codebook and data in the same folder.

But perhaps the most important element for understanding and analyzing the data
is the "var_info" element, so let's take a look at that.

---

# The DDI codebook is the recipe

```{r echo=FALSE}
old_width <- getOption("width")
options(width = 150)
```


```{r}
ddi$var_info
```

```{r echo=FALSE}
options(width = old_width)
```

???
As we can see here, var_info is just data.frame where each row has information 
about one variable, including the variable name, label, description, and value 
labels, as well as where the variable is located in the fixed-width file.

You don't need to remember the details of what's in the DDI object or this 
var_info data.frame, as long as you remember that to read in the data, you must 
point to the DDI codebook, not the data file.

---

# Overview

.greyed-out[


`1.` What is IPUMS?

`2.` What is ipumsr, and why use it?

`3.` How to create a data extract

`4.` Reading data into R

.strong[`5.` Exploring and manipulating metadata]

`6.` Intro to the IPUMS USA API

`7.` IPUMS API use cases

`8.` Q & A

]

---

# What's in my extract again?

We can print the names of our variables:

--

```{r}
names(data)

```


But often variable names aren't self-explanatory.

--

Let's leverage that attached metadata!

???
[Read text on slides]

---

# Available metadata

Variable labels and descriptions:

```{r}
ipums_var_label(ddi, MIGRATE1)

ipums_var_desc(ddi, MIGRATE1) %>% 
  substr(1,450) %>% 
  strwrap(60) 
```

???
So you can see here that ipumsr provides function to extract information from 
the DDI object without the need to slice and dice that "var_info" data.frame I 
showed before. Here we grab the label and description for the variable "Married 
within the past year."

---

# Available metadata

Value labels:

```{r}
ipums_val_labels(ddi, MIGRATE1)
```

???
Similarly, we can print the value labels by pointing to the DDI. We see here 
that the variable married in the last year takes three values: NA, blank, and 
yes.

---


# An interactive view of metadata

```{r, eval=FALSE}
ipums_view(ddi)
```

```{r echo=FALSE, fig.alt="Screenshot of static html page generated by function 'ipums_view', showing variable label and variable description for the variable 'PHONE'.", fig.height=3, fig.width=5}
knitr::include_graphics("images/ipums_view_screenshot_migr.png")
```

???
For a more browsable view of the metadata, the ipums_view() function makes a
nicely-formatted static html page that allows you to browse the metadata
associated with your data extract.

---

# Wrangling value labels

- R doesn't natively support value labels like other statistical packages do

--

- `ipumsr` uses `haven::labelled()` objects to preserve values and labels, but 
  these objects can be tricky to work with
  
--

- `ipumsr` helper functions allow you to leverage info from values and labels

--

- When you are finished manipulating variables, convert to factor using 
`as_factor()` or convert to character or numeric vector

```{r, include = FALSE}
label_before_after <- function(df, before_var, after_var, label_fun) {
  before_var <- rlang::enquo(before_var)
  after_var <- rlang::enquo(after_var)

  
  freqs <- df %>% 
    mutate(val = !!before_var) %>%
    group_by(val) %>%
    summarize(count = n()) 
  
  all_vals <- ipums_val_labels(df, !!before_var) %>% pull(val)
  attributes(all_vals) <- attributes(df %>% pull(!!before_var))
  
  unique_val_df <- dplyr::tibble(val = all_vals) %>%
    mutate(
      after = label_fun(val),
      `before ([val] label)` = as_factor(val, "both")
    ) %>%
    filter(after %in% levels(df %>% pull(!!after_var)) | is.na(after))
  
  attributes(unique_val_df$val) <- NULL
  attributes(freqs$val) <- NULL
  
  out <- left_join(unique_val_df, freqs, by = "val") %>%
    mutate(count = ifelse(is.na(count), 0, count)) %>%
    select(`before ([val] label)`, after, count)
  
  out %>%
    DT::datatable(
      rownames = FALSE, 
      filter = "none", 
      options = list(
        searching = FALSE, 
        scrollY = "200px",
        scrollCollapse = TRUE,
        paging = FALSE,
        bInfo = FALSE
      )
    )
}
```

???
IPUMS value labels don't translate perfectly to R factors. The most important 
difference is that in a factor, values always count up from 1. In IPUMS 
variables, the values themselves often encode meaningful information about a 
nested structure, which we'll see with the education variable below.

Other stats packages are more full featured - for example, R doesn't natively support variables where only some of the values are labelled, or labelled variables with values that aren't all sequential. 

To preserve the value and label `ipumsr` imports labelled variables as
`haven::labelled()` objects which are structured more like labelled variables in
other stats packages.

Luckily ipumsr provides helpers that allow you to use information from both the
value and label to transform your variables into the shape you want.
*We will show you some of these helper functions today*

When you are done transforming your variables, you will probably need to change back to a format native to R to run an analysis because labelled variables aren't supported. To 
do this you'll need to convert back to factor or numeric or char vector using as_factor or zap_label.

---

class: center middle

# Using ipumsr label helper functions

---

# haven::labelled columns at a glance

```{r echo=FALSE}
old_width <- getOption("width")
options(width = 150)
```

- Let's look at the detailed education variable

```{r}
ipums_val_labels(data$EDUCD)

```

???
Here's what the data look like when you read them in. Show detailed education
variable as an example - nested structure.

We see here that values less than 10 indicate missing or no schooling values; 
values between 10 and 19 all capture levels between nursery school and grade 4; 
and so on.

We can take advantage of all of the details given by both the value and label
when we use a labelled variable.

---

# `lbl_collapse()`
- `lbl_collapse()` allows you to take advantage of the hierarchical
  structure of value labels
  
```{r}
ipums_val_labels(data$EDUCD)
```


???
[Read bullet point]


---

# `lbl_collapse()`

- `lbl_collapse()` allows you to take advantage of the hierarchical
  structure of value labels
  
--

```{r}
data$EDUCD2 <- lbl_collapse(data$EDUCD, ~.val %/% 10) %>%
  as_factor(ordered = TRUE)
```


---

# `lbl_collapse()`

- All `ipumsr` helper functions allow you to use `.val` or `.lbl` to refer to 
values or value labels

```{r, eval = FALSE}
data$EDUCD2 <- lbl_collapse(data$EDUCD, `~`.val %/% 10) %>%
  as_factor(ordered = TRUE)

```

--

---

# `lbl_collapse()`

- In this case, we want to collapse the last digit

```{r, eval = FALSE}
data$EDUCD2 <- lbl_collapse(data$EDUCD, ~.val `%/%` 10) %>%
  as_factor(ordered = TRUE)
```

---

# `lbl_collapse()`

```{r}
data$EDUCD2 <- lbl_collapse(data$EDUCD, ~.val %/% 10) %>%
  as_factor(ordered = TRUE)
```

. . .

```{r, echo = FALSE}
label_before_after(
  data, 
  EDUCD, 
  EDUCD2,
  . %>% lbl_collapse(~.val %/% 10) %>%
    as_factor()
)
```

???
[Read bullet point]
The label collapse function allows you to collapse values based on a
hierarchical coding scheme. 

Here we use the integer division operator "percent forward slash percent" to
group values with the same first digit, and label collapse automatically assigns
the label of the smallest original value.

You can
interpret this integer division expression as "how many times does 10 go into
this value?" For original values 0, 1, and 2, the answer is that 10 goes into
the value zero times, so those values all receive the same output value, with
the label coming from the smallest original value. I think the details of what's
going on here take a bit of time to unpack, but hopefully this gives you a sense
of the usefulness and power of this function.

---

# Still too detailed for a graph

```{r}
data$EDUCD %>% 
  lbl_collapse(~.val %/% 10) %>% 
  ipums_val_labels()
```

???
This shows the values and labels that we get by applying lbl_collapse() to 
collapse the last digit. However, if we want to visualize educational 
differences, as we'll do below, this might still be too much detail.

---

# `lbl_relabel()`

- Maybe the education variable is still too specific.

```{r}
college_regex <- "^[123] year(s)? of college$"
data$EDUCD3 <- data$EDUCD %>%
  lbl_collapse(~.val %/% 10) %>% 
  lbl_relabel(
    lbl(2, "Less than High School") ~.val > 0 & .val < 6,
    lbl(3, "High school") ~.lbl == "Grade 12",
    lbl(4, "Some college") ~str_detect(.lbl, college_regex),
    lbl(5, "College or more") ~.val %in% c(10, 11)
  ) %>%
  as_factor()
```


???
That's where `lbl_relabel()` comes in.

Label relabel offers the most power and flexibility by allowing you to create an 
arbitrary number of new labeled values, controlling both the resulting value 
and label, using the special dot val and dot label variables to refer to the 
values and labels of the input vector.

Label relabel is powerful, but it's syntax may be confusing at first glance, so
let's unpack what's going on here. The first line just creates a regular 
expression string that will match either 1, 2, or 3 years of college, and which 
we use a few lines below.

The piped expression takes the EDUCD variable, then applies the same label 
collapse expression we used before to collapse the last digit, resulting in the 
values and labels shown on the previous slide. So, that collapsed education 
variable is the input to the lbl_relabel() call.

Each line in the lbl_relabel() call creates a labeled value using the l-b-l (or 
"label") helper function. So the first line can be read as "assign a value of 
two with a label of "Less than High School" when the input value is greater 
than 0 and less than 6".

---

# `lbl_relabel()`

- Maybe the education variable is still too specific.

```{r eval=FALSE}
college_regex <- "^[123] year(s)? of college$"

data$EDUCD3 <- data$EDUCD %>%
  lbl_collapse(~.val %/% 10) %>% 
  lbl_relabel(
    lbl(2, "Less than High School") ~.val > 0 & .val < 6, #<<
    lbl(3, "High school") ~.lbl == "Grade 12",
    lbl(4, "Some college") ~str_detect(.lbl, college_regex),
    lbl(5, "College or more") ~.val %in% c(10, 11) #<<
  ) %>%
  as_factor()
```


???
Notice that we have examples of using the dot val variable on the first and 
fourth lines...


---

# `lbl_relabel()`

- Maybe the education variable is still too specific.

```{r eval=FALSE}
college_regex <- "^[123] year(s)? of college$"

data$EDUCD3 <- data$EDUCD %>%
  lbl_collapse(~.val %/% 10) %>% 
  lbl_relabel(
    lbl(2, "Less than High School") ~.val > 0 & .val < 6,
    lbl(3, "High school") ~.lbl == "Grade 12", #<<
    lbl(4, "Some college") ~str_detect(.lbl, college_regex), #<<
    lbl(5, "College or more") ~.val %in% c(10, 11)
  ) %>%
  as_factor()

```


???
...and of using the dot label variable on the second and third lines. The 
third line of the lbl_relabel() call can be read as "assign a value of 4 with a 
label of "Some college" when the input label matches our "college_regex" 
regular expression.

---

# `lbl_relabel()`

```{r, eval = FALSE}
levels(data$EDUCD3)
```

```{r, echo = FALSE}
old_width <- getOption("width")
options(width = 50)
levels(data$EDUCD3)
options(width = old_width)
```
???
So, now we have a education variable with fewer categories, which is more
suitable for visualization.

For more on these helper functions, see the "Value labels" vignette, which we 
will link to below.

---

class: center

```{r include=FALSE}
data <- data %>% 
  mutate(
    moved_in_last_year = case_when(
      MIGRATE1 %in% c(0, 9) ~ NA, 
      MIGRATE1 == 1 ~ FALSE, 
      MIGRATE1 %in% 2:4 ~ TRUE
    )
  )
```

```{r migration-graph-1, echo=FALSE, eval=FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT)
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    labs(
      title = "Percentage of people in Puerto Rico who moved in the past year",
      x = NULL,
      y = "%",
      caption = "Source: IPUMS USA PRCS data"
    )
```

```{r migration-graph-1, dpi=300, fig.height=4, fig.width=6, echo = FALSE}
```

Note: To estimate confidence intervals, use variables REPWT or REPTWP

???
Now we're ready to create a line graph of the percentage of people who moved in 
the past year. Notice that we are excluding people with missing values on the 
migration variable from our calculations.

It looks like the percentage of people who had moved in the past year increased 
from about 6% to 8% between 2017 and 2018, possibly as a result of Hurricane 
Maria, which made landfall in September of 2017.

However, it is important to keep in mind that all of the analyses presented here
include only point estimates without any estimation of sampling error. Thus, we
cannot assess the statistical significance of any differences over time or
between groups. To estimate confidence intervals for point estimates from the
ACS or PRCS, use the replicate weight variables REPWT and REPWTP. There are 
instructions for using those variables on the IPUMS USA replicate weight FAQ 
page, which is linked from the variable descriptions for REPWT and REPWTP.

---

```{r migration-graph-2, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(AGE >= 25) %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR, EDUCD3) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~EDUCD3) +
    labs(
      title = "Percentage of people in Puerto Rico who moved in the past year",
      subtitle = "Persons age 25 and older",
      x = NULL,
      y = "%",
      caption = "Source: IPUMS USA PRCS data"
    )
```

???
Now, we can use that nice, collapsed education variable that we just created to 
visualize these trends by level of education. Here we see a more complex picture 
of who was moving during this time period, though again, we would need to 
calculate confidence intervals to know whether these differences over time or by 
education are statistically significant.

---

# Overview

.greyed-out[

`1.` What is IPUMS?

`2.` What is ipumsr, and why use it?

`3.` How to create a data extract

`4.` Reading data into R

`5.` Exploring and manipulating metadata

.strong[`6.` Intro to the IPUMS USA API]

`7.` IPUMS API use cases

`8.` Q & A

]


---

# API Timeline

- Currently in beta testing for IPUMS USA

- IPUMS USA public launch expected mid-2022

- Interested in becoming a beta tester? Email ipums+api@umn.edu

???
The API is not yet publicly available, but is available to volunteer beta 
testers. If you'd like to become a beta tester, email ipums+api@umn.edu and you 
should hear back within about two business days.

IPUMS USA is the first microdata project for which we are adding API support,
but there is already a public API for the IPUMS NHGIS collection, which offers
access to tabular data from the US Census Bureau, as well as corresponding
geographic boundaries. ipumsr does not yet include functions for interacting
with the NHGIS API, but there is a guide to interacting with that API in R,
which we'll share a link to at the end of these slides, and we are currently
working to add that functionality to ipumsr.

---

# What can I do with the IPUMS USA API?

- Define and submit extract requests

--

- Check extract status or "wait" for an extract to finish

--

- Download completed extracts

--

- Get info on past extracts

--

- Share extract definitions

???
So, your next question might be, "what will I be able to do with this API?" 
Here's the high-level overview:

You'll be able to:

Define and submit extract requests.

Check the status of a submitted extract, or have R "wait" for an extract to 
finish by periodically checking the status until it is ready to download.

Download completed data extracts.

Get information on past extracts, including the ability to pull down the 
definition of a previous extract, revise that definition, and resubmit it.

And finally, you can save your extract definition to a JSON file, allowing you 
to share the extract definition with other users for collaboration or 
reproducibility. Saving as JSON makes the definition more easily shareable 
across languages, since R is not the only way to interact with the IPUMS API -- 
you can also call the API using a general purpose tool like curl, and IPUMS is 
developing API client tools for Python in parallel with the R client tools that 
will be part of the ipumsr package.

---

# What can't I do with the IPUMS USA API?

- Bypass the extract system entirely

--

- Explore what data are available

--

- Use all features of the extract system (at least not right away)


???
The other important question to answer is what the API can't do.

Most importantly, it can't deliver data "on demand" -- extracts are still 
created through the same extract system used by the website, which means you 
have to wait for your extract to process before you can download it.

In other words, the API does not create a separate system of accessing IPUMS
data, but rather provides a programmatic way to create and download extracts
through the existing extract system.

Secondly, you can't use the API to explore what data are available from IPUMS. 
We plan to add a public metadata API, but the timeline on that is more unknown.
For now, you must rely on the IPUMS website to explore the variables and 
samples that are available.

A third limitation is that API users will not initially have access to all the
bells and whistles of the extract system, such as the ability to attach 
characteristics of family members such as spouses or children. We plan to add 
these capabilities once the core functionality is well-tested and stable.

---

# How to use your IPUMS USA API key

```{r api-key-screenshot, echo=FALSE, fig.alt="Screenshot of a file named '.Renviron' opened in the RStudio editor, containing one line of text that reads `IPUMS_API_KEY = 'paste-your-api-key-here'`"}
knitr::include_graphics("images/api-key-in-renviron-screenshot.png")
```

???
If you become an API beta tester, you will receive instructions on creating an 
API key. Once you create the key, we recommend storing it in an environment 
variable named "IPUMS_API_KEY" by defining that environment variable in a file 
named ".Renviron" that lives in your user account home directory. You can also 
store it a file named ".Renviron" in your project working directory, but then 
you need to make sure you don't include that file if you share your work with 
others, either directly or through a code repository like GitHub. The rest of 
the API code examples in this presentation assume that we've already stored our 
API key in this environment variable, and thus that we don't need to pass that 
key as an argument when we make API calls.

---

# Overview

.greyed-out[


`1.` What is IPUMS?

`2.` What is ipumsr, and why use it?

`3.` How to create a data extract

`4.` Reading data into R

`5.` Exploring and manipulating metadata

`6.` Intro to the IPUMS USA API

.strong[`7.` IPUMS API use cases]

`8.` Q & A


]

---

# Overview

.greyed-out[


`1.` What is IPUMS?

`2.` What is ipumsr, and why use it?

`3.` How to create a data extract

`4.` Reading data into R

`5.` Exploring and manipulating metadata

`6.` Intro to the IPUMS USA API

.strong[`7a.` API use case 1: Revise your extract]

`8.` Q & A

]

---

# Add HHINCOME to our extract

```{r eval=FALSE}
# Pull down definition of last extract
last_extract <- get_last_extract_info("usa")

# Add HHINCOME, resubmit, and download
ddi <- last_extract %>% 
  revise_extract_micro(vars_to_add = "HHINCOME") %>% 
  submit_extract() %>% 
  wait_for_extract() %>% 
  download_extract() %>% 
  read_ipums_ddi()

data <- read_ipums_micro(ddi)
```


???
After looking at that graph of migration by education, it might have occurred to 
me that I'd like to make a similar graph by level of household income. 
Unfortunately, I didn't include the variable HHINCOME in my extract. This was a 
common occurrence when I was an IPUMS user in grad school: I'd start exploring 
my data and realize that I wanted an additional variable.

Although the IPUMS website already includes a "Revise extract" feature that 
allows you to use a previous extract as a starting point for a new one, ipumsr 
includes functions that use the API to accomplish the same thing right from 
your R session.

[Describe what the code does, but don't go into too much detail]

Not only does this approach save you a little time pointing and clicking through 
the IPUMS USA website, it also streamlines the file download process by 
automatically saving the DDI codebook and data files into your current working 
directory. I think this is an example of how the IPUMS API, at this early stage 
in its development at least, doesn't so much enable you to do things that were 
impossible without the API, but rather provides slightly more efficient and 
seamless ways of completing existing workflows.

Note that you would likely run this code interactively as a convenient way to 
revise, resubmit, and redownload your extract, but you wouldn't want to include 
it in your analysis script, because then you'd be waiting for and redownloading 
your extract every time you reran your analysis.

---

# Update your script with new filename

```{r eval=FALSE}
# Before
ddi <- read_ipums_ddi("usa_00013.xml")
data <- read_ipums_micro(ddi)

# After
ddi <- read_ipums_ddi("usa_00014.xml")
data <- read_ipums_micro(ddi)
```

???
Instead, you would likely hard-code the newly downloaded filename into your 
analysis script, replacing the old filename.

---

```{r echo=FALSE, dpi=300, fig.height=5, fig.width=8}
value_to_quintile <- function(x) {
  cut_points <- quantile(x, probs = c(0.2, 0.4, 0.6, 0.8), na.rm = TRUE)
  cut(
    x, 
    breaks = c(-Inf, cut_points, Inf), 
    labels = c("Lowest", "Lower Middle", "Middle", "Upper Middle", "Highest"),
    ordered_result = TRUE
  )
}

hhincome_quintiles <- data %>% 
  filter(PERNUM == 1 & HHINCOME != 9999999) %>% 
  group_by(YEAR) %>% 
  mutate(hhincome_quintile = value_to_quintile(HHINCOME)) %>% 
  ungroup() %>% 
  select(YEAR, SERIAL, hhincome_quintile)

data <- data %>% 
  left_join(hhincome_quintiles, by = c("YEAR", "SERIAL"))

data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(!is.na(hhincome_quintile)) %>% 
  group_by(YEAR, hhincome_quintile) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~hhincome_quintile) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      subtitle = "Grouped by household income quintile",
      x = NULL,
      y = "%"
    )
```

???
Now we can create a graph similar to the education one grouping instead by
household income quintile. Here we see that persons from households in the
lowest income quintile were more likely to move across this whole time period,
and that the lowest and middle quintiles have a less pronounced jump in
migration in 2018. Again, I want to remind you that these are purely descriptive
trends, and that differences over time and by group may not be statistically
significant.

---

# Overview

.greyed-out[


`1.` What is IPUMS?

`2.` What is ipumsr, and why use it?

`3.` How to create a data extract

`4.` Reading data into R

`5.` Exploring and manipulating metadata

`6.` Intro to the IPUMS USA API

`7a.` API use case 1: Revise your extract

.strong[`7b.` API use case 2: Share your extract]

`8.` Q & A


]



---

# Share your extract (definition)

```{r eval=FALSE}
extract_to_share <- get_last_extract_info("usa")

# Or if you have the extract number:
extract_to_share <- get_extract_info("usa:14")

save_extract_as_json(
  extract_to_share, 
  file = "prcs_migration_extract.json"
)
```

--

- Then share "prcs_migration_extract.json" with collaborators

???
Now let's imagine we've completed our analysis, which might have involved a few 
more extract revisions, and we want to enable our collaborators or other 
researchers to reproduce or build on our analysis. A crucial step toward that 
goal is making sure those other researchers are using the same data that you 
used. However, IPUMS terms prohibit users from freely sharing their data. 
To communicate to other researchers the contents of your IPUMS extract, you 
could describe the variables and samples and other options that you included, 
but the API provides a more efficient and less error-prone way to share your 
data extract, or more exactly, the definition of that extract.

[Describing code...]

The first step in this workflow is using the API to pull down the definition of 
your extract...

...once you've created the json file containing the extract definition, you can 
share it with other researchers in whatever way is convenient.

---

# Clone a shared extract

```{r eval=FALSE}
shared_extract_definition <- define_extract_from_json(
  "prcs_migration_extract.json",
  collection = "usa"
)

ddi <- shared_extract_definition %>% 
  submit_extract() %>% 
  wait_for_extract() %>% 
  download_extract() %>% 
  read_ipums_ddi()

data <- read_ipums_micro(ddi)
```

???
Because the extract definition is saved as JSON, your collaborators don't have 
to use R to consume that file, but if they are using R, they can follow the 
workflow shown here to read in the definition and submit their own extract 
request for an identical dataset.

---

# Overview

.greyed-out[

`1.` What is IPUMS?

`2.` What is ipumsr, and why use it?

`3.` How to create a data extract

`4.` Reading data into R

`5.` Exploring and manipulating metadata

`6.` Intro to the IPUMS USA API

`7a.` API use case 1: Revise your extract

`7b.` API use case 2: Share your extract

.strong[`7c.` API use case 3: Share your analysis]

`8.` Q & A

]

---

class: center

# Save analysis as RMarkdown

```{r echo=FALSE, out.width="85%", fig.alt="Screenshot of RMarkdown file named migration_example_rmarkdown.Rmd opened in the RStudio editor. The code in the file loads required packages, loads data from IPUMS USA extract 14, preps the data in a code chunk that is collapsed in the screenshot, then uses the .tabset RMarkdown notation to create three tabs for graphs of migration overall, by education, and by household income."}
knitr::include_graphics("images/migration-rmarkdown-doc-screenshot.png")
```

???
Now let's say you want to go a step further and share not just your extract 
definition, but your full analysis from start to finish. I'll show a simple 
example here, but this same workflow could be used to share a publication-ready 
analysis and write-up.

A good first step to share your analysis is to put it in an R Markdown document, 
such as the one shown here. If you're not familiar with R Markdown, its a plain 
text format that uses a special syntax to combine R code and formatted text, 
which can then be rendered into HTML, PDF, or even Word document reports.

---

class: center

```{r echo=FALSE, out.width="75%", fig.alt="Screenshot of knitted HTML report titled 'Graphing migration in Puerto Rico 2015-2019 with IPUMS PRCS data' opened in web browser window. The report includes a first level heading that reads 'Migration 2015-2019' and four tabs titled 'Overall', 'By education', 'By household income', and 'By age'. The 'By age' tab is selected and visualizes migration by age, with a line graph for five age groups in separate panels. In the graphs, age groups 0-9, 10-17, and 18-34 are more likely to have moved in the past year than the older age groups across the whole time period, and also show a more pronounced increase from 2017 to 2018."}
knitr::include_graphics("images/knitted-rmarkdown-screenshot-4.png")
```

???
This slide shows the result of rendering that RMarkdown document as an HTML 
report. You can see R Markdown allows you to create reports with nice features 
like a tabbed structure for different plots or report sections. In this report 
I've hidden the code chunks used to load and prepare the data and to create the 
graphs, but you can also include the code in the rendered report.

---

# Instead of hard-coding the extract number...

This only works for you:

```{r eval=FALSE}
ddi <- read_ipums_ddi("usa_00014.xml")
data <- read_ipums_micro(ddi)
```

???
Next, in that R Markdown document that contains your analysis and
interpretation, you want to change the code that reads in the data so that it
will work not only for you, but for those you share it with, who won't already
have the data on their computer. So instead of hard-coding the extract number...

---

# ...download the data if not available locally!

```{r eval=FALSE}
if (!file.exists("prcs_migration_extract.xml")) {
  # Load extract definition from JSON
  prcs_migration_extract <- define_extract_from_json(
    "prcs_migration_extract.json",
    "usa"
  )
  # Submit, wait for, and download extract
  ddi_filename <- submit_extract(prcs_migration_extract) %>% 
    wait_for_extract() %>% 
    download_extract() %>% 
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names
  file.rename(ddi_filename, "prcs_migration_extract.xml")
  file.rename(data_filename, "prcs_migration_extract.dat.gz")
}
```

???

...you include code to download the data if it's not already downloaded and 
available locally.

The code on this slide shows one way to accomplish that. The first thing you'll
notice is that we are checking for the existence of a DDI codebook file that
isn't named with a collection and extract number, such as USA 14 from the
previous slide, because if you are sharing this with someone else, you don't
know what the extract number of their extract will be. If this DDI codebook file
doesn't exist in the working directory, this code will load the extract
definition from the included JSON file, submit a new extract request, download
the extract files, and rename them to match this naming scheme, so that the next
time you run the code, the DDI codebook file will be in your working directory,
and all this code will be skipped.

This code is included in an example R Markdown that we will share a link to, in 
case you'd like to explore it in more detail or use it yourself.

Adding this code to your R Markdown document will allow collaborators to run
your code without making any changes, and we will show an example of that in a
brief live demo at the end of the presentation.

---

# Download the data if not available locally

Now this will work for anyone!

```{r eval=FALSE}
ddi <- read_ipums_ddi("prcs_migration_extract.xml")
data <- read_ipums_micro(
  ddi,
  data_file = "prcs_migration_extract.dat.gz"
)
```


---

# Download the data if not available locally

Now this will work for anyone!

```{r eval=FALSE}
ddi <- read_ipums_ddi("prcs_migration_extract.xml")
data <- read_ipums_micro(
  ddi,
  data_file = "prcs_migration_extract.dat.gz" #<<
)
```

???
Now, since the code in the if statement ensures that the data are available 
locally and have the expected filename, we can read in the DDI and data in a way 
that will work for any researcher with API access. Notice that since we've 
renamed the data file, we need to use the `data_file` argument in 
`read_ipums_micro()` to specify that file name.

---

# Create a GitHub repository

```{r echo=FALSE, eval=TRUE, fig.alt="Screenshot of RStudio Files pane. The user is in a folder named 'puerto-rico-migration' within the Home directory, and that folder contains two files, named 'migration_example_rmarkdown.Rmd' and 'prcs_migration_extract.json'."}
knitr::include_graphics("images/project-directory-screenshot.png")
```

???
So again, you could share these files in whatever way is convenient, but putting 
them in a GitHub repository is a great way to promote open science and 
reproducibility.

At this point, let's assume you are working in RStudio in a folder containing 
two files: your R Markdown analysis document and the JSON definition of your 
extract.

---

# Create a GitHub repository

```{r eval=FALSE}
usethis::create_project(".")
usethis::use_git()
usethis::use_github()
```

--

For more details, check out the free online book
[Happy Git and GitHub for the useR](https://happygitwithr.com)

???
From the starting point on the previous slide, you can get your project on 
GitHub in three R function calls. The first line turns your current working 
directory into an RStudio project, if it isn't one already. The second line 
turns on git version control for your project, again, if you aren't using it 
already, and it optionally will make an initial commit on your behalf. Finally, 
the third line creates a repository on github.com with the same name as your 
project directory, connects that repository to your local project, and pushes 
your committed files up to that GitHub repository.

This workflow assumes that you already have git installed on your computer, that 
you have an account on github.com, and that you have created a GitHub personal 
access token. All these steps are described in the fantastic free online book 
*Happy Git and GitHub for the useR* by Jenny Bryan.

And note that while this workflow doesn't rely on the IPUMS API, it does use the 
GitHub API, and I think the convenience of these functions from the usethis 
package is a good example of the cool stuff APIs make possible.

---

# Create a GitHub repository

```{r echo=FALSE, eval=TRUE, fig.alt="Screenshot of GitHub repository named 'dtburk/puerto-rico-migration' opened in a browser window."}
knitr::include_graphics("images/puerto-rico-migration-github-repo-screenshot.png")
```

???
With those three steps, we now have a GitHub repository that you can point 
anyone you'd like to share your analysis with to. And as we'll show in a minute, 
RStudio tries to make it easy to create a new project directly from a GitHub 
repo, so this a great way to share your work with other R users.

And with that, I'll turn it over to Dan for a brief live demo.

---

# Resources
- Email us: ipums+cran@umn.edu

- Post on the IPUMS User Forum: https://forum.ipums.org/

- This presentation: https://github.com/ipums/ipumsr-psu-api-workshop

- ipumsr website, with vignettes: http://tech.popdata.org/ipumsr/index.html

- IPUMS USA API vignette: https://tech.popdata.org/ipumsr/dev/articles/ipums-api.html

- IPUMS tutorials page: https://www.ipums.org/support/tutorials

- IPUMS NHGIS API documentation: https://developer.ipums.org/docs/workflows/

- Instructions for using replicate weights: https://usa.ipums.org/usa/repwt.shtml

- *Happy Git and GitHub for the useR*: https://happygitwithr.com