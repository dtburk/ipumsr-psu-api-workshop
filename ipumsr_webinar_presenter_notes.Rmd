---
title: "ipumsr webinar presenter notes"
output: html_document
---


# NO HEADING (1)

Thanks to everyone for joining us today, and thanks to Scott for organizing this
workshop and inviting us to talk about "Enhancing reproducibility with the IPUMS
API and the ipumsr package." Before getting to the IPUMS API, we will provide
some helpful context in terms of what data are available from IPUMS, how to
create a data extract without the API, and quick introduction to some of the key
functionality of the ipumsr package.

All the materials for this presentation, including a PDF version of these 
slides, will be available on GitHub, and we'll share the link after the 
workshop, so you shouldn't feel the need to take extensive notes. We should have 
time at the end of our presentation for some Q&A, but you can also put questions 
in the chat during the presentation and one of us will try to answer or add them 
to a list to discuss at the end.

With that, we'll get started by briefly introducing ourselves.


# Who we are (2)

(Name, pronouns, academic field, time with IPUMS)

So, my name is Derek Burk, my pronouns are he/him, I've been working on the
IPUMS International team for the past four years, and my academic training is in
sociology.


# Who we are (3)

Rather than our faces, we thought everyone would appreciate pets.

**A FEW NOTES**

This presentation will be recorded and will be available along with slides so don't worry about trying to take notes.

You can always reach out to through our **github**, or via **user support**.


# Overview (4)




# Overview (5)




# What is IPUMS? (6)

ipums has grown substantially since its first beta release in 1993, 

started with **US census data** has grown to include 9 collections


# Harmonization (7)




# NO HEADING (8)




# NO HEADING (9)




# NO HEADING (10)




# NO HEADING (11)




# NO HEADING (12)




# NO HEADING (13)




# NO HEADING (14)




# NO HEADING (15)

Check out blog using IPUMS Global Health Data


# NO HEADING (16)




# NO HEADING (17)




# NO HEADING (18)




# NO HEADING (19)




# So what is IPUMS? (20)

So ipums really is **data** and a whole lot of it. These 9 different projects interact with different types of data and at different scales but they are united in the use of metadata that helps contextualize the data

*So I know you're asking yourselves..*


# So what is IPUMS? (21)




# Overview (22)




# What is ipumsr? (23)

(Metadata such as value labels, variable labels, and detailed variable 
descriptions.)

Initial API support will be for IPUMS USA, with more projects to follow soon.


# Why use ipumsr? (24)

Regarding "One package": Without ipumsr, you'd need to use a variety of
different approaches from different packages to read in and explore IPUMS


**microdata** from IPUMS: USA, CPS, and International, IPUMS

**aggregate data** (from NHGIS or IHGIS), 
and **IPUMS shapefiles**. ipumsr provides one
package with a consistent interface for working with all these different types
of IPUMS data.

*a one stop shop that makes it easy to work with ipums data*

Regarding "More features": The aforementioned IPUMS API support will be the next
big feature. Another potential new feature is adding tools for properly handling
survey weights. Let us know what would be helpful to you via GitHub or email.


# To run the code in this presentation (25)

Repeat: slides/recording will be available. To run this code yourself you can clone/download the repo from our github. This provides the data extracts you'll need. 

However, you may need to install some additional packages used in the example code below, as shown on the next slide.


# Install R packages (as needed) (26)

reminder, installing packages only needs to happen once, but some packages do update frequently so it can be a good idea to re-install once in a while


# Overview (27)

Dan passes to Derek. Derek switches to a web demo of extract system.


# Overview (28)

Website demo ends, back to slides. Derek passes to Kara.


# Downloading your data extract (29)

Kind of confusing how to save the DDI/.xml file. THIS IS HOW.
DDI is EXTREMELY important, as it contains all the instructions regarding the METADATA

Once your extract is complete, download the data file and the DDI. Downloading
the DDI is a little bit different depending on your browser. On most browsers
you should right-click the file and select “Save As…”. If this saves a file with
a .xml file extension, then you should be ready. However, Safari users must
select “Download Linked File” instead of “Download Linked File As”. On Safari,
selecting the wrong version of these two will download a file with a .html file
extension instead of a .xml extension.

In case anyone was curious, DDI stands for "Data Documentation Initiative" --
the DDI project sets standards for documenting datasets, and the codebooks for
most IPUMS projects follow this standard.

Make sure to save the data and DDI files in the same location.


# Downloading your data extract (30)

The links under "Command Files" contain program-specific code for reading in the
data. The R one contains the code we'll show on the next slide.

This helper code checks that you have ipumsr installed, and if you do, it reads
in the DDI codebook and data into separate objects.


# Read in the data (31)

So you've downloaded both the data and the DDI codebook, and saved them in the 
same folder. Here's how you actually read the data into R.

The first option, and the one I'd recommend, is to read the DDI codebook into an 
object named "ddi" using the `read_ipums_ddi()` function, and then supply that 
ddi object to `read_ipums_micro()`.

Notice that we are supplying the codebook (either already read in
or as a file path), not the data file, to `read_ipums_micro()`. The fact that we
don't point to the actual data file when reading in the data might confusing. 

Want to emphasize that we are pointing to the ddi - not the data file - to read 
it in.

***Change the slides below to just show what's in a ddi + data file, so it
makes sense why we need both (streamlined version of what we did before)


# The data file is just raw ingredients (32)

Looking at the first ten lines of our example data file, it makes sense why you 
can't just tell R "read this data file into a data.frame". The data file itself
doesn't contain any information about what the variables are called or which 
numbers on each line correspond to which variables, let alone what the meaning 
of those values might be.

(In case you're wondering what those blanks are, that's a geography variable 
that is not available in some years of our data.)


# The DDI codebook is the recipe (33)

The DDI codebook tells you what to do with the ingredients in the data file. It
contains a bunch of information about your extract, including the name of the
data file, which is why you don't need to supply the path to the data file if
you saved the codebook and data in the same folder.

But perhaps the most important element for understanding and analyzing the data
is the "var_info" element, so let's take a look at that.


# The DDI codebook is the recipe (34)

As we can see here, var_info is just data.frame where each row has information 
about one variable, including the variable name, label, description, and value 
labels, as well as where the variable is located in the fixed-width file.

You don't need to remember the details of what's in the DDI object or this 
var_info data.frame, as long as you remember that to read in the data, you must 
point to the DDI codebook, not the data file.


# Overview (35)




# What's in my extract again? (36)

[Read text on slides]


# Available metadata (37)

So you can see here that ipumsr provides function to extract information from 
the DDI object without the need to slice and dice that "var_info" data.frame I 
showed before. Here we grab the label and description for the variable "Married 
within the past year."


# Available metadata (38)

Similarly, we can print the value labels by pointing to the DDI. We see here 
that the variable married in the last year takes three values: NA, blank, and 
yes.


# An interactive view of metadata (39)

For a more browsable view of the metadata, the ipums_view() function makes a
nicely-formatted static html page that allows you to browse the metadata
associated with your data extract.


# How do I manipulate metadata? (40)




# Wrangling value labels (41)

IPUMS value labels don't translate perfectly to R factors. The most important 
difference is that in a factor, values always count up from 1. In IPUMS 
variables, the values themselves often encode meaningful information about a 
nested structure, which we'll see with the education variable below.

Other stats packages are more full featured - for example, R doesn't natively support variables where only some of the values are labelled, or labelled variables with values that aren't all sequential. 

To preserve the value and label `ipumsr` imports labelled variables as
`haven::labelled()` objects which are structured more like labelled variables in
other stats packages.

Luckily ipumsr provides helpers that allow you to use information from both the
value and label to transform your variables into the shape you want.
*We will show you some of these helper functions today*

When you are done transforming your variables, you will probably need to change back to a format native to R to run an analysis because labelled variables aren't supported. To 
do this you'll need to convert back to factor or numeric or char vector using as_factor or zap_label.


# Using ipumsr label helper functions (42)




# haven::labelled columns at a glance (43)

Here's what the data look like when you read them in. Show detailed education
variable as an example - nested structure.

We see here that values less than 10 indicate missing or no schooling values; 
values between 10 and 19 all capture levels between nursery school and grade 4; 
and so on.

We can take advantage of all of the details given by both the value and label
when we use a labelled variable.


# `lbl_collapse()` (44)

[Read bullet point]


# `lbl_collapse()` (45)




# `lbl_collapse()` (46)




# `lbl_collapse()` (47)




# `lbl_collapse()` (48)

[Read bullet point]
The label collapse function allows you to collapse values based on a
hierarchical coding scheme. 

Here we use the integer division operator "percent forward slash percent" to
group values with the same first digit, and label collapse automatically assigns
the label of the smallest original value.

You can
interpret this integer division expression as "how many times does 10 go into
this value?" For original values 0, 1, and 2, the answer is that 10 goes into
the value zero times, so those values all receive the same output value, with
the label coming from the smallest original value. I think the details of what's
going on here take a bit of time to unpack, but hopefully this gives you a sense
of the usefulness and power of this function.


# Still too detailed for a graph (49)

This shows the values and labels that we get by applying lbl_collapse() to 
collapse the last digit. However, if we want to visualize educational 
differences, as we'll do below, this might still be too much detail.


# `lbl_relabel()` (50)

That's where `lbl_relabel()` comes in.

Label relabel offers the most power and flexibility by allowing you to create an 
arbitrary number of new labeled values, controlling both the resulting value 
and label, using the special dot val and dot label variables to refer to the 
values and labels of the input vector.

Label relabel is powerful, but it's syntax may be confusing at first glance, so
let's unpack what's going on here. The first line just creates a regular 
expression string that will match either 1, 2, or 3 years of college, and which 
we use a few lines below.

The piped expression takes the EDUCD variable, then applies the same label 
collapse expression we used before to collapse the last digit, resulting in the 
values and labels shown on the previous slide. So, that collapsed education 
variable is the input to the lbl_relabel() call.

Each line in the lbl_relabel() call creates a labeled value using the l-b-l (or 
"label") helper function. So the first line can be read as "assign a value of 
two with a label of "Less than High School" when the input value is greater 
than 0 and less than 6".


# `lbl_relabel()` (51)

Notice that we have examples of using the dot val variable on the first and 
fourth lines...


# `lbl_relabel()` (52)

...and of using the dot label variable on the second and third lines. The 
third line of the lbl_relabel() call can be read as "assign a value of 4 with a 
label of "Some college" when the input label matches our "college_regex" 
regular expression.


# `lbl_relabel()` (53)

So, now we have a education variable with fewer categories, which is more
suitable for visualization.

For more on these helper functions, see the "Value labels" vignette, which we 
will link to below.


# NO HEADING (54)

Now we're ready to create a line graph of the percentage of people who moved in 
the past year. Notice that we are excluding people with missing values on the 
migration variable from our calculations.

It looks like the percentage of people who had moved in the past year increased 
from about 6% to 8% between 2017 and 2018, possibly as a result of Hurricane 
Maria, which made landfall in September of 2017.

However, it is important to keep in mind that all of the analyses presented here
include only point estimates without any estimation of sampling error. Thus, we
cannot assess the statistical significance of any differences over time or
between groups. To estimate confidence intervals for point estimates from the
ACS or PRCS, use the replicate weight variables REPWT and REPWTP. There are 
instructions for using those variables on the IPUMS USA replicate weight FAQ 
page, which is linked from the variable descriptions for REPWT and REPWTP.


# NO HEADING (55)

Now, we can use that nice, collapsed education variable that we just created to 
visualize these trends by level of education. Here we see a more complex picture 
of who was moving during this time period, though again, we would need to 
calculate confidence intervals to know whether these differences over time or by 
education are statistically significant.


# Overview (56)




# API Timeline (57)

The API is not yet publicly available, but is available to volunteer beta 
testers. If you'd like to become a beta tester, email ipums+api@umn.edu and you 
should hear back within about two business days.

IPUMS USA is the first microdata project for which we are adding API support,
but there is already a public API for the IPUMS NHGIS collection, which offers
access to tabular data from the US Census Bureau, as well as corresponding
geographic boundaries. ipumsr does not yet include functions for interacting
with the NHGIS API, but there is a guide to interacting with that API in R,
which we'll share a link to at the end of these slides, and we are currently
working to add that functionality to ipumsr.


# What can I do with the IPUMS USA API? (58)

So, your next question might be, "what will I be able to do with this API?" 
Here's the high-level overview:

You'll be able to:

Define and submit extract requests.

Check the status of a submitted extract, or have R "wait" for an extract to 
finish by periodically checking the status until it is ready to download.

Download completed data extracts.

Get information on past extracts, including the ability to pull down the 
definition of a previous extract, revise that definition, and resubmit it.

And finally, you can save your extract definition to a JSON file, allowing you 
to share the extract definition with other users for collaboration or 
reproducibility. Saving as JSON makes the definition more easily shareable 
across languages, since R is not the only way to interact with the IPUMS API -- 
you can also call the API using a general purpose tool like curl, and IPUMS is 
developing API client tools for Python in parallel with the R client tools that 
will be part of the ipumsr package.


# What can't I do with the IPUMS USA API? (59)

The other important question to answer is what the API can't do.

Most importantly, it can't deliver data "on demand" -- extracts are still 
created through the same extract system used by the website, which means you 
have to wait for your extract to process before you can download it.

In other words, the API does not create a separate system of accessing IPUMS
data, but rather provides a programmatic way to create and download extracts
through the existing extract system.

Secondly, you can't use the API to explore what data are available from IPUMS. 
We plan to add a public metadata API, but the timeline on that is more unknown.
For now, you must rely on the IPUMS website to explore the variables and 
samples that are available.

A third limitation is that API users will not initially have access to all the
bells and whistles of the extract system, such as the ability to attach 
characteristics of family members such as spouses or children. We plan to add 
these capabilities once the core functionality is well-tested and stable.


# How to use your IPUMS USA API key (60)

If you become an API beta tester, you will receive instructions on creating an 
API key. Once you create the key, we recommend storing it in an environment 
variable named "IPUMS_API_KEY" by defining that environment variable in a file 
named ".Renviron" that lives in your user account home directory. You can also 
store it a file named ".Renviron" in your project working directory, but then 
you need to make sure you don't include that file if you share your work with 
others, either directly or through a code repository like GitHub. The rest of 
the API code examples in this presentation assume that we've already stored our 
API key in this environment variable, and thus that we don't need to pass that 
key as an argument when we make API calls.


# Overview (61)




# Overview (62)




# Add HHINCOME to our extract (63)

After looking at that graph of migration by education, it might have occurred to 
me that I'd like to make a similar graph by level of household income. 
Unfortunately, I didn't include the variable HHINCOME in my extract. This was a 
common occurrence when I was an IPUMS user in grad school: I'd start exploring 
my data and realize that I wanted an additional variable.

Although the IPUMS website already includes a "Revise extract" feature that 
allows you to use a previous extract as a starting point for a new one, ipumsr 
includes functions that use the API to accomplish the same thing right from 
your R session.

[Describe what the code does, but don't go into too much detail]

Not only does this approach save you a little time pointing and clicking through 
the IPUMS USA website, it also streamlines the file download process by 
automatically saving the DDI codebook and data files into your current working 
directory. I think this is an example of how the IPUMS API, at this early stage 
in its development at least, doesn't so much enable you to do things that were 
impossible without the API, but rather provides slightly more efficient and 
seamless ways of completing existing workflows.

Note that you would likely run this code interactively as a convenient way to 
revise, resubmit, and redownload your extract, but you wouldn't want to include 
it in your analysis script, because then you'd be waiting for and redownloading 
your extract every time you reran your analysis.


# Update your script with new filename (64)

Instead, you would likely hard-code the newly downloaded filename into your 
analysis script, replacing the old filename.


# NO HEADING (65)

Now we can create a graph similar to the education one grouping instead by
household income quintile. Here we see that persons from households in the
lowest income quintile were more likely to move across this whole time period,
and that the lowest and middle quintiles have a less pronounced jump in
migration in 2018. Again, I want to remind you that these are purely descriptive
trends, and that differences over time and by group may not be statistically
significant.


# How to create the extract from scratch (66)

And just so you don't think the API can only revise extracts created on the 
website, here's how you'd create and download the same extract that Dan made 
earlier, with the addition of the HHINCOME variable.


# Overview (67)




# Share your extract (definition) (68)

Now let's imagine we've completed our analysis, which might have involved a few 
more extract revisions, and we want to enable our collaborators or other 
researchers to reproduce or build on our analysis. A crucial step toward that 
goal is making sure those other researchers are using the same data that you 
used. However, IPUMS terms prohibit users from freely sharing their data. 
To communicate to other researchers the contents of your IPUMS extract, you 
could describe the variables and samples and other options that you included, 
but the API provides a more efficient and less error-prone way to share your 
data extract, or more exactly, the definition of that extract.

[Describing code...]

The first step in this workflow is using the API to pull down the definition of 
your extract...

...once you've created the json file containing the extract definition, you can 
share it with other researchers in whatever way is convenient.


# Clone a shared extract (69)

Because the extract definition is saved as JSON, your collaborators don't have 
to use R to consume that file, but if they are using R, they can follow the 
workflow shown here to read in the definition and submit their own extract 
request for an identical dataset.


# Overview (70)




# Save analysis as RMarkdown (71)

Now let's say you want to go a step further and share not just your extract 
definition, but your full analysis from start to finish. I'll show a simple 
example here, but this same workflow could be used to share a publication-ready 
analysis and write-up.

A good first step to share your analysis is to put it in an R Markdown document, 
such as the one shown here. If you're not familiar with R Markdown, its a plain 
text format that uses a special syntax to combine R code and formatted text, 
which can then be rendered into HTML, PDF, or even Word document reports.


# NO HEADING (72)

This slide shows the result of rendering that RMarkdown document as an HTML 
report. You can see R Markdown allows you to create reports with nice features 
like a tabbed structure for different plots or report sections. In this report 
I've hidden the code chunks used to load and prepare the data and to create the 
graphs, but you can also include the code in the rendered report.


# Instead of hard-coding the extract number... (73)

Next, in that R Markdown document that contains your analysis and
interpretation, you want to change the code that reads in the data so that it
will work not only for you, but for those you share it with, who won't already
have the data on their computer. So instead of hard-coding the extract number...


# ...download the data if not available locally! (74)

...you include code to download the data if it's not already downloaded and 
available locally.

The code on this slide shows one way to accomplish that. The first thing you'll
notice is that we are checking for the existence of a DDI codebook file that
isn't named with a collection and extract number, such as USA 14 from the
previous slide, because if you are sharing this with someone else, you don't
know what the extract number of their extract will be. If this DDI codebook file
doesn't exist in the working directory, this code will load the extract
definition from the included JSON file, submit a new extract request, download
the extract files, and rename them to match this naming scheme, so that the next
time you run the code, the DDI codebook file will be in your working directory,
and all this code will be skipped.

This code is included in an example R Markdown that we will share a link to, in 
case you'd like to explore it in more detail or use it yourself.

Adding this code to your R Markdown document will allow collaborators to run
your code without making any changes, and we will show an example of that in a
brief live demo at the end of the presentation.


# Then hard-code the renamed files (75)




# Then hard-code the renamed files (76)

Now, since the code in the if statement ensures that the data are available 
locally and have the expected filename, we can read in the DDI and data in a way 
that will work for any researcher with API access. Notice that since we've 
renamed the data file, we need to use the `data_file` argument in 
`read_ipums_micro()` to specify that file name.


# Create a GitHub repository (77)

So again, you could share these files in whatever way is convenient, but putting 
them in a GitHub repository is a great way to promote open science and 
reproducibility.

At this point, let's assume you are working in RStudio in a folder containing 
two files: your R Markdown analysis document and the JSON definition of your 
extract.


# Create a GitHub repository (78)

From the starting point on the previous slide, you can get your project on 
GitHub in three R function calls. The first line turns your current working 
directory into an RStudio project, if it isn't one already. The second line 
turns on git version control for your project, again, if you aren't using it 
already, and it optionally will make an initial commit on your behalf. Finally, 
the third line creates a repository on github.com with the same name as your 
project directory, connects that repository to your local project, and pushes 
your committed files up to that GitHub repository.

This workflow assumes that you already have git installed on your computer, that 
you have an account on github.com, and that you have created a GitHub personal 
access token. All these steps are described in the fantastic free online book 
*Happy Git and GitHub for the useR* by Jenny Bryan.

And note that while this workflow doesn't rely on the IPUMS API, it does use the 
GitHub API, and I think the convenience of these functions from the usethis 
package is a good example of the cool stuff APIs make possible.


# Create a GitHub repository (79)

With those three steps, we now have a GitHub repository that you can point 
anyone you'd like to share your analysis with to. And as we'll show in a minute, 
RStudio tries to make it easy to create a new project directly from a GitHub 
repo, so this a great way to share your work with other R users.

And with that, I'll turn it over to Dan for a brief live demo.


# Resources (80)




